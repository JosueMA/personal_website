<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Josué Mendoza</title>
    <link>/post/</link>
    <description>Recent content in Posts on Josué Mendoza</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es-MX</language>
    <copyright>&amp;copy; 2018 Josué Mendoza</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0600</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Probando RMrakdown</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;Este es un documento escrito con &lt;strong&gt;RMarkdown&lt;/strong&gt;. Par más detalles ver &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Puedes incluir un chunk con código de R como en este ejemplo:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;incluyendo-graficas&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Incluyendo gráficas&lt;/h1&gt;
&lt;p&gt;También puedes incluir gráficas. Ver Figura &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; por ejemplo:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Cielo&amp;#39;, &amp;#39;Lado luminoso de la pirámide&amp;#39;, &amp;#39;Lado penumbroso de la pirámide&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;gráfica de pastel sofisticada.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: gráfica de pastel sofisticada.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Rethinking - Exercises</title>
      <link>/post/statistical-rethinking-exercises/</link>
      <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/statistical-rethinking-exercises/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-2---small-worlds-and-large-worlds&#34;&gt;Chapter 2 - Small Worlds and Large Worlds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-3---sampling-the-imaginary&#34;&gt;Chapter 3 - Sampling the Imaginary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-4---linear-models&#34;&gt;Chapter 4 - Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-5---multivariate-linear-models&#34;&gt;Chapter 5 - Multivariate Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-6---overfitting-and-model-comparison&#34;&gt;Chapter 6 - Overfitting and Model Comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-7---interactions&#34;&gt;Chapter 7 - Interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-8---markov-chain-monte-carlo-estimation&#34;&gt;Chapter 8 - Markov chain Monte Carlo Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-9---big-entropy-and-the-generalized-linear-model&#34;&gt;Chapter 9 - Big Entropy and the Generalized Linear Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-10---counting-and-classification&#34;&gt;Chapter 10 - Counting and Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-11---monsters-and-mixtures&#34;&gt;Chapter 11 - Monsters and Mixtures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-12---multilevel-models&#34;&gt;Chapter 12 - Multilevel Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-13---adventures-in-covariance&#34;&gt;Chapter 13 - Adventures in Covariance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-14---missing-data-and-other-opportunities&#34;&gt;Chapter 14 - Missing Data and Other Opportunities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chapter-15---horoscopes&#34;&gt;Chapter 15 - Horoscopes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;chapter-2---small-worlds-and-large-worlds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 2 - Small Worlds and Large Worlds&lt;/h3&gt;
&lt;div id=&#34;easy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;div id=&#34;e1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2E1&lt;/h5&gt;
&lt;p&gt;Which of the expressions below correspond to the statement: &lt;em&gt;the probability of rain on Monday?&lt;/em&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Pr(rain)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Pr(rain|Monday)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Pr(Monday|rain)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Pr(rain|Monday) / Pr(Monday)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;En página 36 se demuestra que la probabilidad conjunta de dos eventos es igual al numerador del teorema.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2E2&lt;/h5&gt;
&lt;p&gt;Which of the following expressions corresponds to the expression: &lt;span class=&#34;math inline&#34;&gt;\(Pr(Monday|rain)\)&lt;/span&gt;?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The probability of rain on Monday.&lt;/li&gt;
&lt;li&gt;The probability of rain, given that it is Monday.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The probability of Monday, given that it is raining.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The probability that is Monday and it is raining.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;e3&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2E3&lt;/h5&gt;
&lt;p&gt;Which of the expressions below correspond to the statement: &lt;em&gt;the probability that it is Monday, given that it is raining?&lt;/em&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Pr(Monday|rain)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Pr(rain|Monday)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Pr(rain|Monday)Pr(Monday)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Pr(rain|Monday)Pr(Monday)/Pr(rain)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Pr(Monday|rain)Pr(rain)/Pr(Monday)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;e4&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2E4&lt;/h5&gt;
&lt;p&gt;The Bayesian statistician Bruno de Finetti (1906–1985) began his book on probability theory with the declaration: “PROBABILITY DOES NOT EXIST.” e capitals appeared in the original, so I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say “the probability of water is 0.7”?&lt;/p&gt;
&lt;p&gt;La probabilidad es sólo un modelo en “small world”, y nos permite formalizar y modelar nuestra incertidumbre y así hacer inferencias (y reducirla con información proveniente de observaciones) tomándolo en cuenta; no obstante, no tenemos acceso al estado real del mundo y nos tendremos que limitar a nuestro conocimiento y el modelo.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;medium&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;div id=&#34;m1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M1&lt;/h5&gt;
&lt;p&gt;Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;W,W,W&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define grid
p_grid &amp;lt;- seq(from = 0 , to = 1 , length.out = 20 )

# define prior
prior &amp;lt;- rep(1 , 20)

# compute likelihood at each value in grid
(likelihood &amp;lt;- dbinom(3, size = 3 , prob = p_grid))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.0000000000 0.0001457938 0.0011663508 0.0039364339 0.0093308062
##  [6] 0.0182242309 0.0314914711 0.0500072897 0.0746464499 0.1062837148
## [11] 0.1457938475 0.1940516110 0.2519317685 0.3203090830 0.4000583175
## [16] 0.4920542353 0.5971715994 0.7162851728 0.8502697186 1.0000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute product of likelihood and prior
(unstd.posterior &amp;lt;- likelihood * prior)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.0000000000 0.0001457938 0.0011663508 0.0039364339 0.0093308062
##  [6] 0.0182242309 0.0314914711 0.0500072897 0.0746464499 0.1062837148
## [11] 0.1457938475 0.1940516110 0.2519317685 0.3203090830 0.4000583175
## [16] 0.4920542353 0.5971715994 0.7162851728 0.8502697186 1.0000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standardize the posterior, so it sums to 1
(posterior &amp;lt;- unstd.posterior / sum(unstd.posterior))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.000000e+00 2.770083e-05 2.216066e-04 7.479224e-04 1.772853e-03
##  [6] 3.462604e-03 5.983380e-03 9.501385e-03 1.418283e-02 2.019391e-02
## [11] 2.770083e-02 3.686981e-02 4.786704e-02 6.085873e-02 7.601108e-02
## [16] 9.349030e-02 1.134626e-01 1.360942e-01 1.615512e-01 1.900000e-01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot
plot( p_grid , posterior , type = &amp;quot;b&amp;quot; ,
    xlab = &amp;quot;probability of water&amp;quot; , ylab = &amp;quot;posterior probability&amp;quot;)
mtext(&amp;quot;20 points&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;W,W,W,L&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define grid
p_grid &amp;lt;- seq(from = 0 , to = 1 , length.out = 20 )

# define prior
prior &amp;lt;- rep(1 , 20)

# compute likelihood at each value in grid
(likelihood &amp;lt;- dbinom(3, size = 4 , prob = p_grid))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.0000000000 0.0005524819 0.0041743081 0.0132595668 0.0294657039
##  [6] 0.0537135228 0.0861871840 0.1263342055 0.1728654630 0.2237551891
## [11] 0.2762409742 0.3268237659 0.3712678693 0.4046009469 0.4211140185
## [16] 0.4143614613 0.3771610101 0.3015937570 0.1790041513 0.0000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute product of likelihood and prior
(unstd.posterior &amp;lt;- likelihood * prior)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.0000000000 0.0005524819 0.0041743081 0.0132595668 0.0294657039
##  [6] 0.0537135228 0.0861871840 0.1263342055 0.1728654630 0.2237551891
## [11] 0.2762409742 0.3268237659 0.3712678693 0.4046009469 0.4211140185
## [16] 0.4143614613 0.3771610101 0.3015937570 0.1790041513 0.0000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standardize the posterior, so it sums to 1
(posterior &amp;lt;- unstd.posterior / sum(unstd.posterior))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.0000000000 0.0001460636 0.0011035915 0.0035055261 0.0077900579
##  [6] 0.0142006265 0.0227859195 0.0333998734 0.0457016732 0.0591557525
## [11] 0.0730317932 0.0864047260 0.0981547300 0.1069672331 0.1113329114
## [16] 0.1095476898 0.0997127416 0.0797344889 0.0473246020 0.0000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot
plot( p_grid , posterior , type = &amp;quot;b&amp;quot; ,
    xlab = &amp;quot;probability of water&amp;quot; , ylab = &amp;quot;posterior probability&amp;quot;)
mtext(&amp;quot;20 points&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;L,W,W,L,W,W,W&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define grid
p_grid &amp;lt;- seq(from = 0 , to = 1 , length.out = 20 )

# define prior
prior &amp;lt;- rep(1 , 20)

# compute likelihood at each value in grid
(likelihood &amp;lt;- dbinom(5, size = 7 , prob = p_grid))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.000000e+00 7.611830e-06 2.172661e-04 1.461471e-03 5.412857e-03
##  [6] 1.438965e-02 3.087358e-02 5.685868e-02 9.314926e-02 1.387256e-01
## [11] 1.902958e-01 2.421517e-01 2.864484e-01 3.140244e-01 3.158816e-01
## [16] 2.854436e-01 2.217106e-01 1.334285e-01 4.439219e-02 0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute product of likelihood and prior
(unstd.posterior &amp;lt;- likelihood * prior)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.000000e+00 7.611830e-06 2.172661e-04 1.461471e-03 5.412857e-03
##  [6] 1.438965e-02 3.087358e-02 5.685868e-02 9.314926e-02 1.387256e-01
## [11] 1.902958e-01 2.421517e-01 2.864484e-01 3.140244e-01 3.158816e-01
## [16] 2.854436e-01 2.217106e-01 1.334285e-01 4.439219e-02 0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standardize the posterior, so it sums to 1
(posterior &amp;lt;- unstd.posterior / sum(unstd.posterior))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.000000e+00 3.205153e-06 9.148535e-05 6.153894e-04 2.279220e-03
##  [6] 6.059124e-03 1.300010e-02 2.394178e-02 3.922284e-02 5.841391e-02
## [11] 8.012882e-02 1.019641e-01 1.206163e-01 1.322279e-01 1.330099e-01
## [16] 1.201932e-01 9.335685e-02 5.618344e-02 1.869245e-02 0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot
plot( p_grid , posterior , type = &amp;quot;b&amp;quot; ,
    xlab = &amp;quot;probability of water&amp;quot; , ylab = &amp;quot;posterior probability&amp;quot;)
mtext(&amp;quot;20 points&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M2&lt;/h5&gt;
&lt;p&gt;Now assume a prior for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; that is equal to zero when &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; 0.5\)&lt;/span&gt; and is a positive constant when &lt;span class=&#34;math inline&#34;&gt;\(p ≥ 0.5\)&lt;/span&gt;. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# New prior
prior &amp;lt;- ifelse( p_grid &amp;lt; 0.5 , 0 , 1 )&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;W,W,W&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;W,W,W,L&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;L,W,W,L,W,W,W&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m3&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M3&lt;/h5&gt;
&lt;p&gt;Suppose there are two globes, one for Earth and one for Mars. e Earth globe is 70% covered in water. e Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (&lt;span class=&#34;math inline&#34;&gt;\(Pr(Earth|land)\)&lt;/span&gt;), is 0.23.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-18-statistical-rethinking-exercises_files/figure-html/decision_tree-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(Earth|Land) = \frac{Pr(Land|Earth) P(Earth)}{Pr(Land|Earth) P(Earth) + Pr(Land|Mars) P(Mars)} = \frac{.3 \times .5}{.3 \times .5 + 1 \times .5}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pr_E = .5 # Pr(Earth)
pr_M = .5 # Pr(Mars)

pr_L_E = .3 # Pr(Land|Earth)
pr_L_M = 1 # Pr(land|Mars)

pr_L = pr_L_E * pr_E + pr_L_M * pr_M # Pr(Land)

# Pr(Earth|land)
(pr_E_L = (pr_L_E * pr_E) / pr_L)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2307692&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;m4&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M4&lt;/h5&gt;
&lt;p&gt;Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. e second card has one black and one white side. e third card has two white sides. Now suppose all three cards are placed in a bag and shu ed. Someone reaches into the bag and pulls out a card and places it at on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. is means counting up the ways that each card could produce the observed data (a black side facing up on the table).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m5&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M5&lt;/h5&gt;
&lt;p&gt;Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m6&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M6&lt;/h5&gt;
&lt;p&gt;Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. A er experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m7&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2M7&lt;/h5&gt;
&lt;p&gt;Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the rst card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. &lt;strong&gt;Hint:&lt;/strong&gt; Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible rst card.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hard&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;div id=&#34;h1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2H1&lt;/h5&gt;
&lt;p&gt;Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. ey look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. ey di er however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of eld research.&lt;/p&gt;
&lt;p&gt;Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2H2&lt;/h5&gt;
&lt;p&gt;Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h3&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2H3&lt;/h5&gt;
&lt;p&gt;Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h4&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2H4&lt;/h5&gt;
&lt;p&gt;A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of di erent types.&lt;/p&gt;
&lt;p&gt;So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. is is the informa- tion you have about the test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability it correctly identifies a species A panda is 0.8.&lt;/li&gt;
&lt;li&gt;The probability it correctly identifies a species B panda is 0.65.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. en redo your calculation, now using the birth data as well.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-3---sampling-the-imaginary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 3 - Sampling the Imaginary&lt;/h3&gt;
&lt;div id=&#34;easy-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;p&gt;These problems use the samples from the posterior distribution for the globe tossing example. This code will give you a specific set of samples, so that you can check your answers exactly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- rep(1 , 1000)
likelihood &amp;lt;- dbinom(6, size = 9, prob = p_grid )
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
set.seed(100)
samples &amp;lt;- sample(p_grid, prob=posterior, size=1e4, replace=TRUE )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the values in &lt;code&gt;samples&lt;/code&gt; to answer the questions that follow.&lt;/p&gt;
&lt;div id=&#34;e1-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E1&lt;/h5&gt;
&lt;p&gt;How much posterior probability lies below &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.2?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e2-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E2&lt;/h5&gt;
&lt;p&gt;How much posterior probability lies above &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.8?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e3-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E3&lt;/h5&gt;
&lt;p&gt;How much posterior probability lies between &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.2 and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.8?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e4-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E4&lt;/h5&gt;
&lt;p&gt;20% of the posterior probability lies below which value of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e5&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E5&lt;/h5&gt;
&lt;p&gt;20% of the posterior probability lies above which value of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e6&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E6&lt;/h5&gt;
&lt;p&gt;Which values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; contain the narrowest interval equal to 66% of the posterior probability?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e7&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3E7&lt;/h5&gt;
&lt;p&gt;Which values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;div id=&#34;m1-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3M1&lt;/h5&gt;
&lt;p&gt;Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same at prior as before.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m2-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3M2&lt;/h5&gt;
&lt;p&gt;Draw 10,000 samples from the grid approximation from above. en use the samples to calculate the 90% HPDI for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m3-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3M3&lt;/h5&gt;
&lt;p&gt;Construct a posterior predictive check for this model and data. is means simulate the distri- bution of samples, averaging over the posterior uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. What is the probability of observing 8 water in 15 tosses?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m4-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3M4&lt;/h5&gt;
&lt;p&gt;Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m5-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3M5&lt;/h5&gt;
&lt;p&gt;Start over at &lt;strong&gt;3M1&lt;/strong&gt;, but now use a prior that is zero below &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.5 and a constant above &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.5. is corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What di erence does the better prior make? If it helps, compare inferences (using both priors) to the true value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.7.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;. The practice problems here all use the data below. These data indicate the gender (male = 1, female = 0) of o cially reported first and second born children in 100 two-child families.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birth1 &amp;lt;- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,
0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,
1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0,
1,0,1,1,1,0,1,1,1,1)

birth2 &amp;lt;- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,
1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,
1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,
0,0,0,1,1,1,0,0,0,0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So for example, the first family in the data reported a boy (1) and then a girl (0). The second family reported a girl (0) and then a boy (1). The third family reported two girls. You can load these two vectors into R’s memory by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rethinking)
data(homeworkch3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use these vectors as data. So for example to compute the total number of boys born across all of these births, you could use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(birth1) + sum(birth2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 111&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;h1-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3H1&lt;/h5&gt;
&lt;p&gt;Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h2-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3H2&lt;/h5&gt;
&lt;p&gt;Using the &lt;code&gt;sample&lt;/code&gt; function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h3-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3H3&lt;/h5&gt;
&lt;p&gt;Use &lt;code&gt;rbinom&lt;/code&gt; to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the &lt;code&gt;rethinking&lt;/code&gt; package) is probably the easiest way in this case. Does it look like the model ts the data well? at is, does the distribution of predictions include the actual observation as a central, likely outcome?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h4-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3H4&lt;/h5&gt;
&lt;p&gt;Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, &lt;code&gt;birth1&lt;/code&gt;. How does the model look in this light?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h5&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3H5&lt;/h5&gt;
&lt;p&gt;The model assumes that sex of first and second births are independent. To check this assump- tion, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-4---linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 4 - Linear Models&lt;/h3&gt;
&lt;div id=&#34;easy-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;div id=&#34;e1-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4E1&lt;/h5&gt;
&lt;p&gt;In the model definition below, which line is the likelihood?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim Normal(\mu, \sigma)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\mu \sim Normal(0, 10)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\sigma \sim Uniform(0, 0)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e2-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4E2&lt;/h5&gt;
&lt;p&gt;In the model de definition just above, how many parameters are in the posterior distribution?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e3-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4E3&lt;/h5&gt;
&lt;p&gt;Using the model de definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e4-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4E4&lt;/h5&gt;
&lt;p&gt;In the model de definition below, which line is the linear model?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim Normal(\mu, \sigma)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\mu = \alpha + \beta x_i\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\alpha \sim Normal(0, 10)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\beta \sim Normal(0,1)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\sigma \sim Uniform(0, 10)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e5-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4E5&lt;/h5&gt;
&lt;p&gt;In the model de definition just above, how many parameters are in the posterior distribution?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;div id=&#34;m1-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4M1&lt;/h5&gt;
&lt;p&gt;For the model de definition below, simulate observed heights from the prior (not the posterior)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim Normal(\mu, \sigma)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\mu \sim Normal(0, 10)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\sigma \sim Uniform(0, 10)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m2-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4M2&lt;/h5&gt;
&lt;p&gt;Translate the model just above into a &lt;code&gt;map&lt;/code&gt; formula.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m3-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4M3&lt;/h5&gt;
&lt;p&gt;Translate the &lt;code&gt;map&lt;/code&gt; model formula below into a mathematical model de definition.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;flist &amp;lt;- alist(
    y ~ dnorm( mu , sigma ),
    mu &amp;lt;- a + b*x,
    a ~ dnorm( 0 , 50 ),
    b ~ dunif( 0 , 10 ),
    sigma ~ dunif( 0 , 50 )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;m4-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4M4&lt;/h5&gt;
&lt;p&gt;A sample of students is measured for heigh teach year for 3years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m5-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4M5&lt;/h5&gt;
&lt;p&gt;Now suppose I tell you that the average height in the first year was 120 cm and that every student got taller each year. Does this information lead you to change your choice of priors? How?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;m6-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4M6&lt;/h5&gt;
&lt;p&gt;Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;div id=&#34;h1-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4H1&lt;/h5&gt;
&lt;p&gt;The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;h2-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4H2&lt;/h5&gt;
&lt;p&gt;Select out all the rows in the &lt;code&gt;Howell1&lt;/code&gt; data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Fit a linear regression to these data, using &lt;code&gt;map&lt;/code&gt;. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Super-impose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;h3-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;4H3&lt;/h5&gt;
&lt;p&gt;Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the &lt;em&gt;logarithm&lt;/em&gt; of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire &lt;code&gt;Howell1&lt;/code&gt; data frame, all 544 rows, adults and non-adults. Fit this model, using quadratic approximation:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[h_i \sim Normal(\mu, \sigma)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\mu_i = \alpha + \beta log(w_i)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\alpha \sim Normal(178, 100)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\beta \sim Normal(0, 100)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\sigma \sim Uniform(0, 50)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(h_i\)&lt;/span&gt; is the height of individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; is the weight (in kg) of individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. The function for computing a natural log in R is just &lt;code&gt;log&lt;/code&gt;. Can you interpret the resulting estimates?&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Begin with this plot:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt; plot(height ~ weight, data = Howell1,
    col = col.alpha(rangi2, 0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-5---multivariate-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 5 - Multivariate Linear Models&lt;/h3&gt;
&lt;div id=&#34;easy-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-6---overfitting-and-model-comparison&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 6 - Overfitting and Model Comparison&lt;/h3&gt;
&lt;div id=&#34;easy-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-7---interactions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 7 - Interactions&lt;/h3&gt;
&lt;div id=&#34;easy-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-8---markov-chain-monte-carlo-estimation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 8 - Markov chain Monte Carlo Estimation&lt;/h3&gt;
&lt;div id=&#34;easy-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-9---big-entropy-and-the-generalized-linear-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 9 - Big Entropy and the Generalized Linear Model&lt;/h3&gt;
&lt;div id=&#34;easy-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-10---counting-and-classification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 10 - Counting and Classification&lt;/h3&gt;
&lt;div id=&#34;easy-8&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-8&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-8&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-11---monsters-and-mixtures&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 11 - Monsters and Mixtures&lt;/h3&gt;
&lt;div id=&#34;easy-9&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-9&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-9&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-12---multilevel-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 12 - Multilevel Models&lt;/h3&gt;
&lt;div id=&#34;easy-10&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-10&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-10&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-13---adventures-in-covariance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 13 - Adventures in Covariance&lt;/h3&gt;
&lt;div id=&#34;easy-11&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-11&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-11&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-14---missing-data-and-other-opportunities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 14 - Missing Data and Other Opportunities&lt;/h3&gt;
&lt;div id=&#34;easy-12&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-12&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-12&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-15---horoscopes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chapter 15 - Horoscopes&lt;/h3&gt;
&lt;div id=&#34;easy-13&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Easy&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;medium-13&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;hard-13&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hard&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
